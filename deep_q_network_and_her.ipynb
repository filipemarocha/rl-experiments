{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_simple\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(10)\n",
      "Box(5, 5, 5, 5, 5, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PutBallInBoxEnvRandom5x5-v0')\n",
    "\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "\n",
    "actions = ['up', 'down', 'left', 'right', 'up-right', 'up-left',\n",
    "    'down-right', 'down-left', 'pick-up', 'put-down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork:\n",
    "    def __init__(self, learning_rate, state_size, action_size, hidden_size, name='QNetwork'):\n",
    "        with tf.variable_scope(name):\n",
    "            # State inputs to the Q-network\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n",
    "\n",
    "            # One hot encode the actions to later choose the Q-value for the action\n",
    "            self.actions_ = tf.placeholder(tf.int32, [None], name='actions')\n",
    "            one_hot_actions = tf.one_hot(self.actions_, action_size)\n",
    "\n",
    "            # Target Q values for training\n",
    "            self.targetQs_ = tf.placeholder(tf.float32, [None], name='target')\n",
    "\n",
    "            # ReLU hidden layers\n",
    "            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_, hidden_size)\n",
    "            self.fc2 = tf.contrib.layers.fully_connected(self.fc1, hidden_size)\n",
    "            \n",
    "            # Linear hidden layer\n",
    "            self.fc3 = tf.contrib.layers.fully_connected(self.fc2, hidden_size,\n",
    "                                                         activation_fn=None)\n",
    "\n",
    "            # Linear output layer\n",
    "            self.output = tf.contrib.layers.fully_connected(self.fc3, action_size,\n",
    "                                                            activation_fn=None)\n",
    "\n",
    "            # Minimize loss: (targetQ - Q)^2\n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, one_hot_actions), axis=1)\n",
    "            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n",
    "            self.opt = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment specific parameters\n",
    "action_size = env.action_space.n\n",
    "state_size = len(env.observation_space.shape)\n",
    "\n",
    "# Training parameters\n",
    "train_episodes = 2000         # max number of episodes to learn from\n",
    "max_steps = 1000              # max steps in an episode\n",
    "gamma = 0.95                  # future reward discount\n",
    "\n",
    "# Exploration parameters\n",
    "explore_start = 1.0           # exploration probability at start\n",
    "explore_stop = 0.1            # minimum exploration probability\n",
    "decay_rate = 0.00005          # exponential decay rate for exploration prob\n",
    "\n",
    "# Network parameters\n",
    "hidden_size = 100             # number of units in each Q-network hidden layer\n",
    "learning_rate = 0.0001        # Q-network learning rate\n",
    "\n",
    "# Memory parameters\n",
    "memory_size = 50000           # memory capacity\n",
    "batch_size = 5000             # experience mini-batch size\n",
    "pretrain_length = batch_size  # number experiences to pretrain the memory\n",
    "\n",
    "# Update Target Network\n",
    "c_steps = 100                  # every c steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory:\n",
    "    def __init__(self, max_size = 1000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "    # experience: (state, action, reward, next_state)\n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        idx = np.random.choice(np.arange(len(self.buffer)),\n",
    "                               size=batch_size,\n",
    "                               replace=False)\n",
    "        return [self.buffer[ii] for ii in idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the simulation\n",
    "state, reward, done, _ = env.reset()\n",
    "\n",
    "memory = Memory(max_size=memory_size)\n",
    "\n",
    "# Make random actions and store the experiences to pre-fill the memory\n",
    "for ii in range(pretrain_length):\n",
    "    # Uncomment the line below to watch the simulation\n",
    "    #env.render()\n",
    "\n",
    "    # Make a random action\n",
    "    action = env.action_space.sample()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "\n",
    "        # Start new episode\n",
    "        state, reward, done, _ = env.reset()\n",
    "\n",
    "    else:\n",
    "        # Add experience to memory\n",
    "        memory.add((state, action, reward, next_state))\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateTargetNetwork(tfVars):\n",
    "    total_vars = len(tfVars)\n",
    "    op_holder = []\n",
    "    for idx,var in enumerate(tfVars[0:total_vars//2]):\n",
    "        op_holder.append(tfVars[idx+total_vars//2].assign(var.value()))\n",
    "    return op_holder\n",
    "\n",
    "def updateTarget(op_holder,sess):\n",
    "    for op in op_holder:\n",
    "        sess.run(op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model following DQN algorithm\n",
    "tf.reset_default_graph()\n",
    "mainQN = QNetwork(name='main', state_size=state_size, action_size=action_size,\n",
    "                  hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "targetQN = QNetwork(name='target', state_size=state_size, action_size=action_size,\n",
    "                    hidden_size=hidden_size, learning_rate=learning_rate)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "trainables = tf.trainable_variables()\n",
    "\n",
    "targetOps = updateTargetNetwork(trainables)\n",
    "\n",
    "cum_avg_reward_lst = []\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "\n",
    "    step = 0\n",
    "    total_reward = 0\n",
    "    for ep in range(1, train_episodes):\n",
    "        state, reward, done, _ = env.reset()\n",
    "        t = 0\n",
    "        while t < max_steps:\n",
    "            step += 1\n",
    "            # Uncomment this next line to watch the training\n",
    "            #env.render()\n",
    "\n",
    "            # Explore or Exploit\n",
    "            explore_p = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*step)\n",
    "            if explore_p > np.random.rand():\n",
    "                # Make a random action\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                # Get action from Q-network\n",
    "                feed = {mainQN.inputs_: state.reshape((1, *state.shape))}\n",
    "                Qs = sess.run(mainQN.output, feed_dict=feed)\n",
    "                action = np.argmax(Qs)\n",
    "\n",
    "            # Take action, get new state and reward\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "\n",
    "            else:\n",
    "                # Add experience to memory\n",
    "                memory.add((state, action, reward, next_state))\n",
    "                state = next_state\n",
    "                t += 1\n",
    "\n",
    "            # Sample mini-batch from memory\n",
    "            batch = memory.sample(batch_size)\n",
    "            states = np.array([each[0] for each in batch])\n",
    "            actions = np.array([each[1] for each in batch])\n",
    "            rewards = np.array([each[2] for each in batch])\n",
    "            next_states = np.array([each[3] for each in batch])\n",
    "\n",
    "            # Train network\n",
    "            target_Qs = sess.run(targetQN.output, feed_dict={targetQN.inputs_: next_states})\n",
    "\n",
    "            if done:\n",
    "                targets = rewards\n",
    "            else:\n",
    "                targets = rewards + gamma * np.max(target_Qs, axis=1)\n",
    "\n",
    "            loss, _ = sess.run([mainQN.loss, mainQN.opt],\n",
    "                                feed_dict={mainQN.inputs_: states,\n",
    "                                           mainQN.targetQs_: targets,\n",
    "                                           mainQN.actions_: actions})\n",
    "            # After every C steps copy Q network to target network periodically\n",
    "            # use target network to calculate target Qs\n",
    "            if  t % c_steps == 0:\n",
    "               updateTarget(targetOps,sess)\n",
    "\n",
    "            if done: break\n",
    "        cum_avg_reward = total_reward/(ep)\n",
    "        cum_avg_reward_lst.append(cum_avg_reward)\n",
    "\n",
    "    saver.save(sess, \"checkpoints/putballinbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cum_avg_reward_lst)\n",
    "plt.ylabel('Avg cumulative reward')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/putballinbox\n",
      "Episode: 1\n",
      "Reached the goal after 13 timesteps\n",
      "88.0\n",
      "Episode: 2\n",
      "Reached the goal after 7 timesteps\n",
      "91.0\n",
      "Episode: 3\n",
      "Didnt reach the goal after 20 timesteps\n",
      "54.0\n",
      "Episode: 4\n",
      "Reached the goal after 14 timesteps\n",
      "62.25\n",
      "Episode: 5\n",
      "Didnt reach the goal after 20 timesteps\n",
      "45.8\n",
      "Episode: 6\n",
      "Didnt reach the goal after 20 timesteps\n",
      "34.833333333333336\n",
      "Episode: 7\n",
      "Reached the goal after 16 timesteps\n",
      "42.0\n",
      "Episode: 8\n",
      "Reached the goal after 13 timesteps\n",
      "47.75\n",
      "Episode: 9\n",
      "Reached the goal after 16 timesteps\n",
      "51.888888888888886\n",
      "Episode: 10\n",
      "Reached the goal after 16 timesteps\n",
      "55.2\n"
     ]
    }
   ],
   "source": [
    "# Test the trained DQN model\n",
    "import time\n",
    "import copy\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Load meta graph and restore weights\n",
    "    saver = tf.train.import_meta_graph('checkpoints/putballinbox.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('checkpoints/'))\n",
    "\n",
    "    graph = tf.get_default_graph()\n",
    "    inputs_tensor = graph.get_tensor_by_name(\"main/inputs:0\")\n",
    "    output_tensor = graph.get_tensor_by_name(\"main/fully_connected_3/BiasAdd:0\")\n",
    "\n",
    "\n",
    "    target_output_tensor = tf.contrib.copy_graph.get_copied_op(output_tensor, graph)\n",
    "\n",
    "    total_reward = 0\n",
    "    n_episodes = 10\n",
    "    n_steps = 20\n",
    "\n",
    "    for i_episode in range(n_episodes):\n",
    "        print('Episode: ' + str(i_episode+1))\n",
    "        state,_,_,_ = env.reset()\n",
    "        for t in range(n_steps):\n",
    "            env.render()\n",
    "            feed = {inputs_tensor: state.reshape((1, *state.shape))}\n",
    "            Qs = sess.run(target_output_tensor, feed_dict=feed)\n",
    "            action = np.argmax(Qs)\n",
    "            next_state, r, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += r\n",
    "            if done:\n",
    "                print(\"Reached the goal after {} timesteps\".format(t+1))\n",
    "                break\n",
    "            time.sleep(1)\n",
    "        if not done:\n",
    "            print(\"Didnt reach the goal after {} timesteps\".format(n_steps))\n",
    "        print(total_reward/(i_episode + 1))\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO: HER (Hindsight Experience Replay)\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
